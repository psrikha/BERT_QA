{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "class SquadDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.contexts = []\n",
    "        self.questions = []\n",
    "        self.answers = []\n",
    "        self.answer_starts = []\n",
    "        self.answer_ends = []\n",
    "\n",
    "        with open(file_path, 'r', encoding='cp1252') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader) # skip header row\n",
    "            for row in reader:\n",
    "    \n",
    "                self.contexts.append(row[0])\n",
    "                self.questions.append(row[1])\n",
    "                self.answers.append(row[2])\n",
    "                self.answer_starts.append(int(row[3]))\n",
    "                self.answer_ends.append(int(row[4]))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'context': self.contexts[idx],\n",
    "            'question': self.questions[idx],\n",
    "            'answer': self.answers[idx],\n",
    "            'answer_start': self.answer_starts[idx],\n",
    "            'answer_end': self.answer_ends[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SquadDataset('sample_squad_train_data.csv')\n",
    "print(len(train_dataset))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "test_dataset = SquadDataset('sample_squad_test_data.csv')\n",
    "print(len(test_dataset))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train(model, data_loader, optimizer, device, ac_stps):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    total_loss = 0\n",
    "    \n",
    "    scaler = GradScaler()  # initialize the GradScaler object\n",
    "    bc = 0\n",
    "\n",
    "    for data in data_loader:\n",
    "        # Move data to device\n",
    "        inputs = tokenizer(\n",
    "            data['context'],\n",
    "            data['question'],\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            stride=128,\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        inputs['input_ids']\n",
    "        inputs['input_ids'].char_to_token(data['answer_start'])\n",
    "        \n",
    "        st_pos = data['answer_start'].to(device)\n",
    "        end_pos = data['answer_end'].to(device)\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():  # enable automatic mixed precision\n",
    "            outputs = model(**inputs, st_pos=st_pos, end_pos=end_pos)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()  # scale the loss and perform backward pass\n",
    "        \n",
    "        bc += 1\n",
    "        if bc % ac_stps == 0:\n",
    "            scaler.step(optimizer)  # update the model weights\n",
    "            scaler.update()  # update the GradScaler for the next iteration\n",
    "            optimizer.zero_grad()  # clear gradients\n",
    "            \n",
    "        lv = loss.item()\n",
    "        \n",
    "        if str(lv) == 'nan':\n",
    "            lv = 0\n",
    "\n",
    "        total_loss += lv\n",
    "\n",
    "    if bc % ac_stps != 0:\n",
    "        scaler.step(optimizer)  # update the model weights\n",
    "        scaler.update()  # update the GradScaler for the next iteration\n",
    "        optimizer.zero_grad()  # clear gradients\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, optimizer, device):    \n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    scaler = GradScaler()  # initialize the GradScaler object\n",
    "    \n",
    "    for data in data_loader:\n",
    "        # Move data to device\n",
    "        inputs = tokenizer(\n",
    "            data['context'],\n",
    "            data['question'],\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            stride=128,\n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        st_pos = data['answer_start'].to(device)\n",
    "        end_pos = data['answer_end'].to(device)\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():  # enable automatic mixed precision\n",
    "            outputs = model(**inputs, st_pos=st_pos, end_pos=end_pos)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        lv = loss.item()\n",
    "        \n",
    "        if str(lv) == 'nan':\n",
    "            lv = 0\n",
    "        \n",
    "        valid_loss += lv\n",
    "\n",
    "    return valid_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 , train loss 5.662377052009106, test loss 5.720812129974365\n",
      "Epoch 2 , train loss 5.437935434281826, test loss 5.669600734710693\n",
      "Epoch 3 , train loss 5.194169580936432, test loss 5.722679519653321\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', max_length=512)\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "model = model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, device, ac_stps=2)\n",
    "    test_loss = test(model, test_loader, optimizer, device)\n",
    "    print(f'Epoch {epoch+1} , train loss {train_loss}, test loss {test_loss}')\n",
    "    # Update learning rate\n",
    "    scheduler.step(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 , train loss 5.717407636344433, test loss 5.78319902420044\n",
      "Epoch 2 , train loss 5.684388391673565, test loss 5.789040679931641\n",
      "Epoch 3 , train loss 5.421862803399563, test loss 5.853338603973389\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
    "\n",
    "# Initialize the DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased', max_length=512)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "model = model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, device, ac_stps=2)\n",
    "    test_loss = test(model, test_loader, optimizer, device)\n",
    "    print(f'Epoch {epoch+1} , train loss {train_loss}, test loss {test_loss}')\n",
    "\n",
    "    scheduler.step(test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(model, tokenizer, context, question):\n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(question, context, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    input_ids = inputs['input_ids'].squeeze()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(**inputs)\n",
    "\n",
    "    # Get predicted answer\n",
    "    start_idx = torch.argmax(output.start_logits)\n",
    "    end_idx = torch.argmax(output.end_logits) + 1\n",
    "    \n",
    "    if end_idx < start_idx:\n",
    "        # Swap the indices if end_idx is less than start_idx\n",
    "        start_idx, end_idx = end_idx, start_idx\n",
    "    \n",
    "    answer = tokenizer.decode(input_ids[start_idx:end_idx])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0') tensor(10, device='cuda:0')\n",
      "tensor([  101,  2054,  2515,  1996,  4419,  5376,  2058,  1029,   102,  1996,\n",
      "         4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,  1012,   102],\n",
      "       device='cuda:0')\n",
      "[CLS] what does the fox jump over? [SEP] the\n"
     ]
    }
   ],
   "source": [
    "context = \"The quick brown fox jumps over the lazy dog.\"\n",
    "question = \"What does the fox jump over?\"\n",
    "answer = predict_answer(model, tokenizer, context, question)\n",
    "print(answer)  # Output: \"the lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
